{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Agentic Job Targeting Workflow for Data Science**"
      ],
      "metadata": {
        "id": "0Bbdw8ZfbOjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cassandra Maldonado"
      ],
      "metadata": {
        "id": "Q8Pxv_qkbYVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from yattag import Doc, indent\n",
        "    import PyPDF2\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"yattag\", \"PyPDF2\", \"nltk\"])\n",
        "    from yattag import Doc, indent\n",
        "    import PyPDF2\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "\n",
        "# NLTK.\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    print(\"NLTK data skipped.\")"
      ],
      "metadata": {
        "id": "2KPMpAqzNm3u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Structure for the resume sections.\n",
        "class ResumeSection:\n",
        "    name: str\n",
        "    email: str\n",
        "    phone: str\n",
        "    summary: str\n",
        "    experience: List[Dict[str, Any]]\n",
        "    education: List[Dict[str, str]]\n",
        "    skills: List[str]\n",
        "    projects: List[Dict[str, Any]]\n",
        "\n",
        "# Resume formatter that creates a better output from the data.\n",
        "class Resumeformatter:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def formatting_resume(self, resume_data: Dict, job_requirements: Dict, template='professional') -> str:\n",
        "\n",
        "        # Cleaning the data.\n",
        "        enhanced_data = self.info_resume_data(resume_data, job_requirements)\n",
        "\n",
        "        # Generating HTML with a template.\n",
        "        html_content = self.html_with_template(enhanced_data, template)\n",
        "\n",
        "        return html_content\n",
        "\n",
        "# Clean and improve the resume info.\n",
        "    def info_resume_data(self, resume_data: Dict, job_requirements: Dict) -> Dict:\n",
        "        enhanced = {}\n",
        "\n",
        "        # Name.\n",
        "        enhanced['name'] = self.cleanning_name(resume_data.get('name', 'Professional'))\n",
        "\n",
        "        # Contact info.\n",
        "        enhanced['email'] = self.clean_email(resume_data.get('email', ''))\n",
        "        enhanced['phone'] = self.clean_phone(resume_data.get('phone', ''))\n",
        "\n",
        "        # Improved summary with job keywords.\n",
        "        enhanced['summary'] = self._summary(\n",
        "            resume_data.get('summary', ''),\n",
        "            job_requirements\n",
        "        )\n",
        "\n",
        "        # Organizing experience.\n",
        "        enhanced['experience'] = self.cleaning_experience(resume_data.get('experience', []))\n",
        "\n",
        "        # Organizing education.\n",
        "        enhanced['education'] = self.cleaning_education(resume_data.get('education', []))\n",
        "\n",
        "        # Skills.\n",
        "        enhanced['skills'] = self.organize_skills(\n",
        "            resume_data.get('skills', []),\n",
        "            job_requirements\n",
        "        )\n",
        "\n",
        "        # Projects.\n",
        "        enhanced['projects'] = self.cleaning_projects(resume_data.get('projects', []))\n",
        "\n",
        "        return enhanced\n",
        "\n",
        "# Cleaning the name field.\n",
        "    def cleanning_name(self, name: str) -> str:\n",
        "        if not name or name.lower() == 'candidate':\n",
        "            return \"CASSANDRA M. SULLIVAN\"\n",
        "\n",
        "        name = name.strip()\n",
        "\n",
        "        # If it looks like contact info got mixed in I extract just the name part.\n",
        "        if '@' in name or '(' in name:\n",
        "            parts = name.split()\n",
        "            clean_parts = []\n",
        "            for part in parts:\n",
        "                if '@' not in part and '(' not in part and not part.isdigit():\n",
        "                    clean_parts.append(part)\n",
        "            name = ' '.join(clean_parts[:4])\n",
        "\n",
        "        return name if name else \"CASSANDRA M. SULLIVAN\"\n",
        "\n",
        "# Cleaning the email.\n",
        "    def clean_email(self, email: str) -> str:\n",
        "        if not email:\n",
        "            return \"\"\n",
        "\n",
        "        email = email.split('|')[0].strip()\n",
        "\n",
        "        if '@' in email and '.' in email:\n",
        "            return email\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "# Cleanining the phone.\n",
        "    def clean_phone(self, phone: str) -> str:\n",
        "        if not phone:\n",
        "            return \"\"\n",
        "\n",
        "        phone = re.sub(r'[^\\d\\(\\)\\-\\.\\s\\+]', '', phone)\n",
        "\n",
        "        return phone.strip()\n",
        "\n",
        "# Imrpoving the summary with the job keywords.\n",
        "    def _summary(self, summary: str, job_requirements: Dict) -> str:\n",
        "        if not summary or len(summary) < 50:\n",
        "            summary = \"\"\"Data scientist with proven expertise in machine learning and generative AI,\n",
        "            skilled in developing automated evaluation pipelines and fine-tuning LLMs. Brings robust\n",
        "            experience in Python, ML/NLP libraries, and A/B testing to optimize model performance at scale.\n",
        "            Recognized for translating complex datasets into actionable, real-world insights.\"\"\"\n",
        "\n",
        "        # Cleaning up the summary.\n",
        "        if len(summary) > 500:\n",
        "            sentences = summary.split('.')\n",
        "            clean_sentences = []\n",
        "            for sentence in sentences[:3]:\n",
        "                sentence = sentence.strip()\n",
        "                if (len(sentence) > 20 and len(sentence) < 200 and\n",
        "                    not any(x in sentence.lower() for x in ['phone', 'email', 'education', 'experience'])):\n",
        "                    clean_sentences.append(sentence)\n",
        "\n",
        "            if clean_sentences:\n",
        "                summary = '. '.join(clean_sentences) + '.'\n",
        "\n",
        "        # Adding the job relevant info.\n",
        "        job_skills = job_requirements.get('skills', []) + job_requirements.get('tools', [])\n",
        "        if job_skills:\n",
        "            top_skills = ', '.join(job_skills[:3])\n",
        "            if 'generative ai' not in summary.lower() and any('ai' in skill.lower() for skill in job_skills):\n",
        "                summary += f\" Specialized in {top_skills} with focus on autonomous systems and model deployment.\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "# Cleaning the experience.\n",
        "    def cleaning_experience(self, experience: List[Dict]) -> List[Dict]:\n",
        "        cleaned = []\n",
        "\n",
        "        for exp in experience:\n",
        "            if not exp.get('company') or exp.get('company') == 'Previous Organization':\n",
        "                continue\n",
        "\n",
        "            clean_exp = {\n",
        "                'title': exp.get('title', '').strip(),\n",
        "                'company': exp.get('company', '').strip(),\n",
        "                'duration': exp.get('duration', '').strip(),\n",
        "                'achievements': []\n",
        "            }\n",
        "\n",
        "            # Cleaning achievements.\n",
        "            for achievement in exp.get('achievements', []):\n",
        "                if isinstance(achievement, str) and len(achievement.strip()) > 15:\n",
        "                    clean_achievement = achievement.strip()\n",
        "                    clean_achievement = clean_achievement.lstrip('•-* ')\n",
        "                    clean_exp['achievements'].append(clean_achievement)\n",
        "\n",
        "            if clean_exp['company'] and clean_exp['title']:\n",
        "                cleaned.append(clean_exp)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "# Cleaning education.\n",
        "    def cleaning_education(self, education: List[Dict]) -> List[Dict]:\n",
        "        cleaned = []\n",
        "\n",
        "        for edu in education:\n",
        "            if edu.get('degree') and len(edu.get('degree', '')) > 5:\n",
        "                clean_edu = {\n",
        "                    'school': edu.get('school', '').strip(),\n",
        "                    'degree': edu.get('degree', '').strip(),\n",
        "                    'date': edu.get('date', '').strip(),\n",
        "                    'details': edu.get('details', '').strip()\n",
        "                }\n",
        "                cleaned.append(clean_edu)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "# Organizing the skills into categories.\n",
        "    def organize_skills(self, skills: List[str], job_requirements: Dict) -> Dict[str, List[str]]:\n",
        "        categories = {\n",
        "            'Programming & Scripting': [],\n",
        "            'Machine Learning & AI': [],\n",
        "            'Data Analysis & Visualization': [],\n",
        "            'Statistical Methods': [],\n",
        "            'Cloud & Infrastructure': []\n",
        "        }\n",
        "\n",
        "        # Keyword mappings.\n",
        "        category_keywords = {\n",
        "            'Programming & Scripting': ['python', 'r', 'sql', 'git', 'jupyter', 'pandas', 'numpy', 'matplotlib'],\n",
        "            'Machine Learning & AI': ['machine learning', 'ai', 'ml', 'tensorflow', 'pytorch', 'xgboost', 'random forest', 'neural', 'nlp', 'generative', 'llm', 'deep learning'],\n",
        "            'Data Analysis & Visualization': ['tableau', 'visualization', 'dashboard', 'analytics', 'streamlit', 'matplotlib'],\n",
        "            'Statistical Methods': ['statistics', 'bayesian', 'time series', 'forecasting', 'causal inference', 'a/b testing'],\n",
        "            'Cloud & Infrastructure': ['aws', 'azure', 'gcp', 'docker', 'kubernetes', 'snowflake']\n",
        "        }\n",
        "\n",
        "        # Categorizing the skills.\n",
        "        for skill in skills:\n",
        "            if not skill or len(skill.strip()) < 2:\n",
        "                continue\n",
        "\n",
        "            skill_clean = skill.strip()\n",
        "            skill_lower = skill_clean.lower()\n",
        "            categorized = False\n",
        "\n",
        "            for category, keywords in category_keywords.items():\n",
        "                if any(keyword in skill_lower for keyword in keywords):\n",
        "                    if skill_clean not in categories[category]:\n",
        "                        categories[category].append(skill_clean)\n",
        "                    categorized = True\n",
        "                    break\n",
        "\n",
        "            # Default category for uncategorized skills.\n",
        "            if not categorized and len(skill_clean) < 20:\n",
        "                categories['Programming & Scripting'].append(skill_clean)\n",
        "\n",
        "        # Removing empty categories and duplicates.\n",
        "        cleaned_categories = {}\n",
        "        for category, skill_list in categories.items():\n",
        "            if skill_list:\n",
        "                cleaned_categories[category] = list(set(skill_list))\n",
        "\n",
        "        return cleaned_categories\n",
        "\n",
        "# Cleaning project entries.\n",
        "    def cleaning_projects(self, projects: List[Dict]) -> List[Dict]:\n",
        "        cleaned = []\n",
        "\n",
        "        for project in projects:\n",
        "            if project.get('name') and len(project.get('name', '')) > 3:\n",
        "                clean_project = {\n",
        "                    'name': project.get('name', '').strip(),\n",
        "                    'duration': project.get('duration', '').strip(),\n",
        "                    'description': project.get('description', '').strip(),\n",
        "                    'technologies': project.get('technologies', [])\n",
        "                }\n",
        "\n",
        "                # Cleaning the description.\n",
        "                if clean_project['description']:\n",
        "                    desc = clean_project['description']\n",
        "                    desc = desc.lstrip('•-* ')\n",
        "                    if len(desc) > 300:\n",
        "                        desc = desc[:300] + '...'\n",
        "                    clean_project['description'] = desc\n",
        "\n",
        "                cleaned.append(clean_project)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "# Generating the HTML with a template.\n",
        "    def html_with_template(self, data: Dict, template: str) -> str:\n",
        "        return self.setting_template()(data)\n",
        "\n",
        "    def setting_template(self):\n",
        "        def template(data):\n",
        "            html = f'''<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>{data[\"name\"]} - Resume</title>\n",
        "    <style>\n",
        "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
        "        body {{\n",
        "            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, sans-serif;\n",
        "            line-height: 1.6;\n",
        "            color: #333;\n",
        "            background: #f8f9fa;\n",
        "            padding: 20px;\n",
        "        }}\n",
        "        .container {{\n",
        "            max-width: 900px;\n",
        "            margin: 0 auto;\n",
        "            background: white;\n",
        "            border-radius: 10px;\n",
        "            overflow: hidden;\n",
        "            box-shadow: 0 10px 30px rgba(0,0,0,0.1);\n",
        "        }}\n",
        "        .header {{\n",
        "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "            color: white;\n",
        "            padding: 40px;\n",
        "            text-align: center;\n",
        "        }}\n",
        "        .header h1 {{\n",
        "            font-size: 2.8em;\n",
        "            font-weight: 300;\n",
        "            margin-bottom: 10px;\n",
        "            letter-spacing: 2px;\n",
        "        }}\n",
        "        .header p {{\n",
        "            font-size: 1.1em;\n",
        "            opacity: 0.9;\n",
        "        }}\n",
        "        .content {{ padding: 40px; }}\n",
        "        .section {{ margin-bottom: 40px; }}\n",
        "        .section h2 {{\n",
        "            color: #667eea;\n",
        "            font-size: 1.4em;\n",
        "            margin-bottom: 20px;\n",
        "            padding-bottom: 10px;\n",
        "            border-bottom: 2px solid #667eea;\n",
        "            font-weight: 600;\n",
        "        }}\n",
        "        .summary {{\n",
        "            font-size: 1.1em;\n",
        "            line-height: 1.8;\n",
        "            color: #555;\n",
        "            background: #f8f9fa;\n",
        "            padding: 25px;\n",
        "            border-radius: 8px;\n",
        "            border-left: 4px solid #667eea;\n",
        "        }}\n",
        "        .experience-item, .education-item, .project-item {{\n",
        "            margin-bottom: 30px;\n",
        "            padding: 25px;\n",
        "            background: #f8f9fa;\n",
        "            border-radius: 8px;\n",
        "            border-left: 4px solid #28a745;\n",
        "        }}\n",
        "        .job-title, .degree, .project-title {{\n",
        "            font-size: 1.2em;\n",
        "            font-weight: 600;\n",
        "            color: #333;\n",
        "            margin-bottom: 8px;\n",
        "        }}\n",
        "        .company, .school {{\n",
        "            font-weight: 600;\n",
        "            color: #28a745;\n",
        "            margin-bottom: 5px;\n",
        "        }}\n",
        "        .duration {{\n",
        "            color: #666;\n",
        "            font-style: italic;\n",
        "            margin-bottom: 15px;\n",
        "        }}\n",
        "        .achievements {{\n",
        "            list-style: none;\n",
        "            margin-top: 15px;\n",
        "        }}\n",
        "        .achievements li {{\n",
        "            margin: 10px 0;\n",
        "            padding-left: 20px;\n",
        "            position: relative;\n",
        "        }}\n",
        "        .achievements li:before {{\n",
        "            content: \"▶\";\n",
        "            color: #28a745;\n",
        "            font-weight: bold;\n",
        "            position: absolute;\n",
        "            left: 0;\n",
        "        }}\n",
        "        .skills-grid {{\n",
        "            display: grid;\n",
        "            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n",
        "            gap: 25px;\n",
        "        }}\n",
        "        .skill-category {{\n",
        "            background: #f8f9fa;\n",
        "            padding: 20px;\n",
        "            border-radius: 8px;\n",
        "            border-left: 4px solid #667eea;\n",
        "        }}\n",
        "        .category-title {{\n",
        "            font-weight: 600;\n",
        "            color: #333;\n",
        "            margin-bottom: 15px;\n",
        "            font-size: 1.1em;\n",
        "        }}\n",
        "        .skill-list {{\n",
        "            display: flex;\n",
        "            flex-wrap: wrap;\n",
        "            gap: 8px;\n",
        "        }}\n",
        "        .skill-tag {{\n",
        "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
        "            color: white;\n",
        "            padding: 6px 14px;\n",
        "            border-radius: 20px;\n",
        "            font-size: 0.9em;\n",
        "            font-weight: 500;\n",
        "        }}\n",
        "        .project-description {{\n",
        "            margin: 15px 0;\n",
        "            line-height: 1.7;\n",
        "        }}\n",
        "        .technologies {{\n",
        "            margin-top: 12px;\n",
        "        }}\n",
        "        .tech-label {{\n",
        "            font-weight: 600;\n",
        "            color: #667eea;\n",
        "        }}\n",
        "        .tech-list {{\n",
        "            color: #666;\n",
        "        }}\n",
        "        @media print {{\n",
        "            body {{ background: white; padding: 0; }}\n",
        "            .container {{ box-shadow: none; }}\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <header class=\"header\">\n",
        "            <h1>{data[\"name\"]}</h1>\n",
        "            <p>'''\n",
        "\n",
        "            contact_parts = []\n",
        "            if data.get(\"phone\"):\n",
        "                contact_parts.append(data[\"phone\"])\n",
        "            if data.get(\"email\"):\n",
        "                contact_parts.append(data[\"email\"])\n",
        "            html += \" | \".join(contact_parts)\n",
        "\n",
        "            html += f'''</p>\n",
        "            <div class=\"links\">\n",
        "                <a href=\"https://github.com/CassandraMaldonado\" target=\"_blank\">🔗 GitHub</a>\n",
        "                <a href=\"https://www.linkedin.com/in/cassandra-msullivan/\" target=\"_blank\">🔗 LinkedIn</a>\n",
        "            </div>\n",
        "        </header>\n",
        "\n",
        "        <div class=\"content\">'''\n",
        "\n",
        "            # Summary section.\n",
        "            if data.get(\"summary\"):\n",
        "                html += f'''\n",
        "            <section class=\"section\">\n",
        "                <h2>Professional Summary</h2>\n",
        "                <div class=\"summary\">{data[\"summary\"]}</div>\n",
        "            </section>'''\n",
        "\n",
        "            # Experience section.\n",
        "            if data.get(\"experience\"):\n",
        "                html += '''\n",
        "            <section class=\"section\">\n",
        "                <h2>Professional Experience</h2>'''\n",
        "                for exp in data[\"experience\"]:\n",
        "                    html += f'''\n",
        "                <div class=\"experience-item\">\n",
        "                    <div class=\"job-title\">{exp.get(\"title\", \"\")}</div>\n",
        "                    <div class=\"company\">{exp.get(\"company\", \"\")}</div>\n",
        "                    <div class=\"duration\">{exp.get(\"duration\", \"\")}</div>'''\n",
        "                    if exp.get(\"achievements\"):\n",
        "                        html += '<ul class=\"achievements\">'\n",
        "                        for achievement in exp[\"achievements\"]:\n",
        "                            html += f'<li>{achievement}</li>'\n",
        "                        html += '</ul>'\n",
        "                    html += '</div>'\n",
        "                html += '</section>'\n",
        "\n",
        "            # Education section.\n",
        "            if data.get(\"education\"):\n",
        "                html += '''\n",
        "            <section class=\"section\">\n",
        "                <h2>Education</h2>'''\n",
        "                for edu in data[\"education\"]:\n",
        "                    html += f'''\n",
        "                <div class=\"education-item\">\n",
        "                    <div class=\"degree\">{edu.get(\"degree\", \"\")}</div>\n",
        "                    <div class=\"school\">{edu.get(\"school\", \"\")}</div>\n",
        "                    <div class=\"duration\">{edu.get(\"date\", \"\")}</div>'''\n",
        "                    if edu.get(\"details\"):\n",
        "                        html += f'<div style=\"margin-top: 10px; color: #666;\">{edu[\"details\"]}</div>'\n",
        "                    html += '</div>'\n",
        "                html += '</section>'\n",
        "\n",
        "            # Projects section.\n",
        "            if data.get(\"projects\"):\n",
        "                html += '''\n",
        "            <section class=\"section\">\n",
        "                <h2>Key Projects</h2>'''\n",
        "                for project in data[\"projects\"]:\n",
        "                    html += f'''\n",
        "                <div class=\"project-item\">\n",
        "                    <div class=\"project-title\">{project.get(\"name\", \"\")}</div>'''\n",
        "                    if project.get(\"duration\"):\n",
        "                        html += f'<div class=\"duration\">{project[\"duration\"]}</div>'\n",
        "                    if project.get(\"description\"):\n",
        "                        html += f'<div class=\"project-description\">{project[\"description\"]}</div>'\n",
        "                    if project.get(\"technologies\"):\n",
        "                        html += f'''<div class=\"technologies\">\n",
        "                            <span class=\"tech-label\">Technologies:</span>\n",
        "                            <span class=\"tech-list\">{\", \".join(project[\"technologies\"])}</span>\n",
        "                        </div>'''\n",
        "                    html += '</div>'\n",
        "                html += '</section>'\n",
        "\n",
        "            # Skills section.\n",
        "            if data.get(\"skills\"):\n",
        "                html += '''\n",
        "            <section class=\"section\">\n",
        "                <h2>Technical Skills</h2>\n",
        "                <div class=\"skills-grid\">'''\n",
        "                for category, skills in data[\"skills\"].items():\n",
        "                    if skills:\n",
        "                        html += f'''\n",
        "                    <div class=\"skill-category\">\n",
        "                        <div class=\"category-title\">{category}</div>\n",
        "                        <div class=\"skill-list\">'''\n",
        "                        for skill in skills:\n",
        "                            html += f'<span class=\"skill-tag\">{skill}</span>'\n",
        "                        html += '</div></div>'\n",
        "                html += '</div></section>'\n",
        "\n",
        "            html += '''\n",
        "        </div>\n",
        "    </div>\n",
        "</body>\n",
        "</html>'''\n",
        "            return html\n",
        "        return template\n",
        "\n",
        "# Creating the file upload widget.\n",
        "def upload_resume_widget():\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Upload your resume (PDF, DOCX, or TXT).\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            return filename\n",
        "        return None\n",
        "    except ImportError:\n",
        "        print(\"Please upload your resume file.\")\n",
        "        import os\n",
        "        files = [f for f in os.listdir('.') if f.lower().endswith(('.pdf', '.docx', '.txt'))]\n",
        "        if files:\n",
        "            print(f\"Found: {files}\")\n",
        "            return files[0]\n",
        "        return None\n",
        "\n",
        "# Parsing the resume.\n",
        "def resume_parser(file_path: str) -> str:\n",
        "    try:\n",
        "        if file_path.lower().endswith('.pdf'):\n",
        "            import PyPDF2\n",
        "            with open(file_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                return text\n",
        "        elif file_path.lower().endswith('.txt'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}.\")\n",
        "        return \"\"\n",
        "\n",
        "# Getting the job description from the user.\n",
        "def interactive_job_input():\n",
        "    print(\"\\n Paste the job description.\")\n",
        "    print(\"When finished, type 'DONE' on a new line:\")\n",
        "\n",
        "    lines = []\n",
        "    while True:\n",
        "        try:\n",
        "            line = input()\n",
        "            if line.strip().upper() == 'DONE':\n",
        "                break\n",
        "            lines.append(line)\n",
        "        except KeyboardInterrupt:\n",
        "            return None\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def format_resume(resume_data, job_requirements, output_filename=None):\n",
        "\n",
        "    if not output_filename:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        output_filename = f\"datascience_resume_{timestamp}.html\"\n",
        "\n",
        "    formatter = Resumeformatter()\n",
        "\n",
        "    # Converting the resume object to dict.\n",
        "    if hasattr(resume_data, '__dict__'):\n",
        "        resume_dict = {\n",
        "            'name': getattr(resume_data, 'name', ''),\n",
        "            'email': getattr(resume_data, 'email', ''),\n",
        "            'phone': getattr(resume_data, 'phone', ''),\n",
        "            'summary': getattr(resume_data, 'summary', ''),\n",
        "            'experience': getattr(resume_data, 'experience', []),\n",
        "            'education': getattr(resume_data, 'education', []),\n",
        "            'skills': getattr(resume_data, 'skills', []),\n",
        "            'projects': getattr(resume_data, 'projects', [])\n",
        "        }\n",
        "    else:\n",
        "        resume_dict = resume_data\n",
        "\n",
        "    # Converting the job requirements to dict.\n",
        "    if hasattr(job_requirements, '__dict__'):\n",
        "        job_dict = {\n",
        "            'skills': getattr(job_requirements, 'skills', []),\n",
        "            'tools': getattr(job_requirements, 'tools', []),\n",
        "            'keywords': getattr(job_requirements, 'keywords', [])\n",
        "        }\n",
        "    else:\n",
        "        job_dict = job_requirements\n",
        "\n",
        "    # HTML.\n",
        "    html_content = formatter.formatting_resume(resume_dict, job_dict, 'professional')\n",
        "\n",
        "    # Saving.\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"New resume generated: {output_filename}\")\n",
        "    print(f\"Name: {resume_dict.get('name', 'Unknown')}\")\n",
        "    print(f\"Experience entries: {len(resume_dict.get('experience', []))}\")\n",
        "    print(f\"Education entries: {len(resume_dict.get('education', []))}\")\n",
        "    print(f\"Projects: {len(resume_dict.get('projects', []))}\")\n",
        "    print(f\"Skill categories: {len(resume_dict.get('skills', {})) if isinstance(resume_dict.get('skills', {}), dict) else 'Multiple'}\")\n",
        "\n",
        "    return output_filename"
      ],
      "metadata": {
        "id": "6D8lKYMumNEh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_workflow():\n",
        "\n",
        "    print(\"Resume workflow.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Getting the resume.\n",
        "    resume_file = upload_resume_widget()\n",
        "    if not resume_file:\n",
        "        return None\n",
        "\n",
        "    # Parsing the text.\n",
        "    resume_text = resume_parser(resume_file)\n",
        "    if not resume_text:\n",
        "        return None\n",
        "\n",
        "    print(f\"Extracted {len(resume_text)} characters from the file.\")\n",
        "\n",
        "    # Getting the job description.\n",
        "    jd_text = interactive_job_input()\n",
        "    if not jd_text:\n",
        "        return None\n",
        "\n",
        "    # Manual extraction based on known content.\n",
        "    experience = []\n",
        "    education = []\n",
        "\n",
        "    # Looking for specific text chunks that contain job info.\n",
        "    text_lower = resume_text.lower()\n",
        "\n",
        "    # Mexico Central Bank job.\n",
        "    if 'mexico central bank' in text_lower:\n",
        "        print(\"Found Mexico Central Bank job.\")\n",
        "        experience.append({\n",
        "            \"title\": \"Machine Learning & Data Science Economist\",\n",
        "            \"company\": \"Mexico Central Bank\",\n",
        "            \"duration\": \"May 2022 - Sep 2024\",\n",
        "            \"achievements\": [\n",
        "                \"Developed forecasting models (XGBoost, ARIMA, Prophet) that improved regional economic predictions by 15%.\",\n",
        "                \"Applied causal inference (DiD, matching) and A/B testing to evaluate the impact of policy interventions.\",\n",
        "                \"Conducted incrementality analysis to isolate true effects of programs on regional growth.\",\n",
        "                \"Automated SQL pipelines while integrating generative AI tools to streamline data evaluation processes.\",\n",
        "                \"Presented findings in policy briefings and national economic reports.\"\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Universidad job.\n",
        "    if 'universidad' in text_lower and 'nuevo león' in text_lower:\n",
        "        print(\"Found Universidad job.\")\n",
        "        experience.append({\n",
        "            \"title\": \"Consultant\",\n",
        "            \"company\": \"Universidad Autónoma de Nuevo León (University-Industry Relations Office)\",\n",
        "            \"duration\": \"Sep 2021 - May 2022\",\n",
        "            \"achievements\": [\n",
        "                \"Designed and deployed Python-based KPI dashboards to track cross-departmental performance, contributing to $500K MXN in operational savings.\",\n",
        "                \"Leveraged A/B testing and uplift modeling in data analysis to isolate the incremental effects of pricing strategies on customer conversion.\"\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Secretary job.\n",
        "    if 'secretary' in text_lower and 'finance' in text_lower:\n",
        "        print(\"Found Secretary of Finance job.\")\n",
        "        experience.append({\n",
        "            \"title\": \"Financial Planning Analyst\",\n",
        "            \"company\": \"Secretary of Finance and General Treasury of Nuevo León (State Gov. Office)\",\n",
        "            \"duration\": \"Jun 2020 - Aug 2021\",\n",
        "            \"achievements\": [\n",
        "                \"Developed R-based forecasting models that improved budget accuracy by 23%.\",\n",
        "                \"Automated SQL pipelines to streamline debt tracking, reducing servicing costs by 0.6%.\",\n",
        "                \"Utilized causal inference (synthetic controls, quasi-experiments) to evaluate the impact of fiscal reforms.\",\n",
        "                \"Facilitated evidence-based decision-making through real-time financial reporting.\"\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Education.\n",
        "    if 'university of chicago' in text_lower:\n",
        "        print(\"Found University of Chicago job.\")\n",
        "        education.append({\n",
        "            \"school\": \"University of Chicago, Physical Sciences Division\",\n",
        "            \"degree\": \"Master of Science, Applied Data Science\",\n",
        "            \"date\": \"Dec 2025\",\n",
        "            \"details\": \"GPA: Magna Cum Laude • Data Science Institute Merit Scholarship\"\n",
        "        })\n",
        "\n",
        "    if 'instituto' in text_lower and 'monterrey' in text_lower:\n",
        "        print(\"Found Monterrey job.\")\n",
        "        education.append({\n",
        "            \"school\": \"Instituto Tecnológico y de Estudios Superiores de Monterrey\",\n",
        "            \"degree\": \"Bachelor of Arts, Economics\",\n",
        "            \"date\": \"Dec 2019\",\n",
        "            \"details\": \"GPA: Magna Cum Laude\"\n",
        "        })\n",
        "\n",
        "    print(f\"\\n Extraction Results:\")\n",
        "    print(f\"Experience entries: {len(experience)}.\")\n",
        "    print(f\"Education entries: {len(education)}.\")\n",
        "\n",
        "    # Creating the resume.\n",
        "    resume_data = ResumeSection(\n",
        "        name=\"CASSANDRA M. SULLIVAN\",\n",
        "        email=\"caseymr96@gmail.com\",\n",
        "        phone=\"(415) 286-1896\",\n",
        "        summary=\"\"\"Data scientist with proven expertise in machine learning and generative AI, skilled in developing automated evaluation pipelines and fine-tuning LLMs. Brings robust experience in Python, ML/NLP libraries, and A/B testing to optimize model performance at scale. Recognized for translating complex datasets into actionable, real-world insights and integrating advanced ML techniques within quality assurance frameworks.\"\"\",\n",
        "        experience=experience,\n",
        "        education=education,\n",
        "        skills=[\n",
        "            \"Python\", \"R\", \"SQL\", \"STATA\", \"Git\", \"Jupyter\", \"Pandas\", \"NumPy\",\n",
        "            \"Matplotlib\", \"Scikit-Learn\", \"Machine Learning\", \"Deep Learning\",\n",
        "            \"Generative AI\", \"LLMs\", \"VAEs\", \"Classification\", \"Regression\",\n",
        "            \"XGBoost\", \"Random Forest\", \"A/B Testing\", \"NLP\", \"TensorFlow\",\n",
        "            \"PyTorch\", \"Statistics\", \"Bayesian Inference\", \"Causal Inference\",\n",
        "            \"Time Series Forecasting\", \"Tableau\", \"Streamlit\"\n",
        "        ],\n",
        "        projects=[\n",
        "            {\n",
        "                \"name\": \"Healthcare LLM (Inference Analytics)\",\n",
        "                \"duration\": \"Mar 2025 - Present\",\n",
        "                \"description\": \"Collaborating on a healthcare-specialized LLM fine-tuned with Reinforcement Learning from EHRs and clinical notes. Focused on prompt engineering, dataset preparation, and reward modeling.\",\n",
        "                \"technologies\": [\"Python\", \"Reinforcement Learning\", \"LLM\", \"EHRs\"]\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"AirfareCast: Airline Fare Forecasting\",\n",
        "                \"duration\": \"Mar 2025\",\n",
        "                \"description\": \"Built machine learning models (XGBoost, Random Forest) to predict flight prices. Deployed an interactive Streamlit dashboard to visualize fare trends and optimize booking decisions.\",\n",
        "                \"technologies\": [\"Python\", \"XGBoost\", \"Random Forest\", \"Streamlit\"]\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Conditional VAE for Age-Controlled Face Generation\",\n",
        "                \"duration\": \"Jan 2025\",\n",
        "                \"description\": \"Designed a Conditional Variational Autoencoder (CVAE) to simulate age-based facial transformations. Implemented data preprocessing and achieved latent space disentanglement.\",\n",
        "                \"technologies\": [\"Python\", \"CVAE\", \"TensorFlow\", \"Computer Vision\"]\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Customer Behavior Analysis Pipeline\",\n",
        "                \"duration\": \"Nov 2024\",\n",
        "                \"description\": \"Built a SQL-Python pipeline to analyze 6.75M+ e-commerce records using A/B testing and time series analysis.\",\n",
        "                \"technologies\": [\"Python\", \"SQL\", \"A/B Testing\", \"Time Series\"]\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"John List Voltage Research Program\",\n",
        "                \"duration\": \"Sep 2024 - Present\",\n",
        "                \"description\": \"Used Bayesian modeling and causal inference to quantify real-world treatment effects in behavioral experiments, evaluated incremental outcomes from randomized interventions.\",\n",
        "                \"technologies\": [\"Python\", \"Bayesian Modeling\", \"Causal Inference\", \"R\"]\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(f\"\\n Job Requirements Analysis.\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    job_requirements = {\n",
        "        'skills': [],\n",
        "        'tools': [],\n",
        "        'keywords': []\n",
        "    }\n",
        "\n",
        "    jd_lower = jd_text.lower()\n",
        "\n",
        "    # Extracting ML/AI skills from the job description.\n",
        "    ml_keywords = ['machine learning', 'deep learning', 'ai', 'generative ai', 'neural networks', 'nlp', 'autonomous driving', 'generative models', 'synthetic data']\n",
        "    tech_keywords = ['python', 'tensorflow', 'pytorch', 'sql', 'docker', 'kubernetes']\n",
        "\n",
        "    for keyword in ml_keywords:\n",
        "        if keyword in jd_lower:\n",
        "            job_requirements['skills'].append(keyword.title())\n",
        "\n",
        "    for keyword in tech_keywords:\n",
        "        if keyword in jd_lower:\n",
        "            job_requirements['tools'].append(keyword.title())\n",
        "\n",
        "    # Extracting keywords.\n",
        "    import re\n",
        "    words = re.findall(r'\\b[a-zA-Z]{4,}\\b', jd_lower)\n",
        "    stop_words = {'the', 'and', 'for', 'with', 'that', 'this', 'will', 'from', 'they', 'have', 'your', 'our'}\n",
        "    filtered_words = [w for w in words if w not in stop_words]\n",
        "    word_counts = {}\n",
        "    for word in filtered_words:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "    # Getting the most common keywords.\n",
        "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    job_requirements['keywords'] = [word for word, count in sorted_words[:15] if count > 1]\n",
        "\n",
        "    print(f\"Found job skills: {job_requirements['skills']}\")\n",
        "    print(f\"Found job tools: {job_requirements['tools']}\")\n",
        "    print(f\"Found keywords: {job_requirements['keywords'][:5]}...\")\n",
        "\n",
        "    beautiful_file = format_resume(resume_data, job_requirements)\n",
        "\n",
        "    # Generating the interview talking points.\n",
        "    print(f\"\\n Interview Prep.\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    interview_points = interview_talking_points(resume_data, job_requirements)\n",
        "\n",
        "    print(f\"New resume: {beautiful_file}\")\n",
        "    print(f\"{len(resume_data.experience)} experience entries extracted.\")\n",
        "    print(f\"{len(resume_data.projects)} projects extracted.\")\n",
        "    print(f\"{len(resume_data.education)} education entries extracted.\")\n",
        "    print(f\"{len(interview_points)} interview talking points generated.\")\n",
        "\n",
        "    print(f\"\\n Interview talking points.\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, point in enumerate(interview_points, 1):\n",
        "        print(f\"\\n{i}. {point}\")\n",
        "\n",
        "    return resume_data, job_requirements, beautiful_file\n",
        "\n",
        "def interview_talking_points(resume_data, job_requirements):\n",
        "    points = []\n",
        "\n",
        "    # Technical expertise.\n",
        "    skills = job_requirements.get('skills', [])\n",
        "    if skills:\n",
        "        top_skills = ', '.join(skills[:3])\n",
        "        points.append(f\"Technical Deep Dive: Prepare a detailed walkthrough of your {top_skills} experience. Use your Healthcare LLM project as an example, explain the technical approach, challenges with EHR data, and how you implemented Reinforcement Learning for fine-tuning.\")\n",
        "\n",
        "    # Problem-solving with examples.\n",
        "    points.append(\"Problem-Solving Examples: Prepare STAR method stories for: 1) Your Mexico Central Bank forecasting models that improved predictions by 15%, what obstacles did you overcome? 2) The Universidad KPI dashboard that saved $500K MXN, how did you identify the key metrics? 3) A time when your initial ML approach failed and how you pivoted.\")\n",
        "\n",
        "    # Career progression.\n",
        "    points.append(\"Career Progression Story: Highlight your growth from Financial Planning Analyst to ML & Data Science Economist. Emphasize: increasing technical complexity (R forecasting → Python ML pipelines → Generative AI), expanding scope (budget accuracy → policy impact → healthcare applications), and leadership development.\")\n",
        "\n",
        "    # Collaboration.\n",
        "    points.append(\"Stakeholder Communication: Prepare examples of: • Presenting complex ML findings to policy makers at Mexico Central Bank • Translating technical limitations to university administrators • Working with cross-departmental teams on KPI dashboards • How you balance technical accuracy with business needs.\")\n",
        "\n",
        "    # Research.\n",
        "    if any('research' in skill.lower() for skill in job_requirements.get('keywords', [])):\n",
        "        points.append(\"Research Excellence: Discuss your John List Voltage Research Program work with Bayesian modeling and causal inference. Explain how you bridge academic research with practical applications, and your approach to experimental design in behavioral studies.\")\n",
        "\n",
        "    if any('autonomous' in keyword.lower() for keyword in job_requirements.get('keywords', [])):\n",
        "        points.append(\"Autonomous Systems Insight: Connect your generative AI and ML pipeline experience to autonomous driving challenges. Discuss data quality issues you've solved, model validation approaches from your forecasting work, and how your A/B testing expertise applies to autonomous system evaluation.\")\n",
        "\n",
        "    # Business impact.\n",
        "    points.append(\"Quantified Business Impact: Prepare specific metrics: • 15% improvement in economic predictions (Mexico Central Bank) • $500K MXN operational savings (Universidad) • 23% budget accuracy improvement (Treasury) • 6.75M+ records analyzed (Customer Behavior project) • Multiple ongoing research projects showing sustained impact.\")\n",
        "\n",
        "    # Questions.\n",
        "    points.append(\"Strategic Questions: Ask thoughtful questions like: • 'What are the biggest challenges in scaling generative models for autonomous systems?' • 'How does the team balance research exploration with production requirements?' • 'What's your approach to ensuring safety and reliability in ML model deployment?' • 'How do you measure success for generative AI initiatives in this domain?'\")\n",
        "\n",
        "    return points\n",
        "\n",
        "def quick_resume_workflow():\n",
        "    return run_workflow()\n",
        "\n",
        "def print_usage_instructions():\n",
        "    print(\"Run: run_workflow()\")\n",
        "\n",
        "print_usage_instructions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOXiCQCZmRMr",
        "outputId": "3f2fd7c4-a8d6-4431-ed8c-e034f84f36f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: run_workflow()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_workflow()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2oE64OiL-fbq",
        "outputId": "a30d6933-c9c7-4e24-f3c4-3d995d131e0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume workflow.\n",
            "--------------------------------------------------\n",
            "Upload your resume (PDF, DOCX, or TXT).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8f4ee48b-2ab2-4bce-9edd-c1397be9b89e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8f4ee48b-2ab2-4bce-9edd-c1397be9b89e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Cassandra_Sullivan_DataScience.pdf to Cassandra_Sullivan_DataScience.pdf\n",
            "Extracted 4406 characters from the file.\n",
            "\n",
            " Paste the job description.\n",
            "When finished, type 'DONE' on a new line:\n",
            " Define and execute the ML roadmap for synthetic data generation using generative AI, evolving both model and infrastructure to meet the training and evaluation needs of Zoox’s autonomous driving solution. Lead the development of generative models from small scale objects to complete scenarios, from research all the way to deployment. Design effective model architectures and sophisticated training techniques, leveraging all the inputs from our sensor stack and the overall large scale data we have at Zoox. Collaborate with perception, planning, safety, simulation, and systems teams to integrate your models into our offline pipelines. Validate and optimize your solutions using real-world driving scenarios, directly contributing to the safety and reliability of Zoox's autonomous system.\n",
            "DONE\n",
            "Found Mexico Central Bank job.\n",
            "Found Universidad job.\n",
            "Found Secretary of Finance job.\n",
            "Found University of Chicago job.\n",
            "Found Monterrey job.\n",
            "\n",
            " Extraction Results:\n",
            "Experience entries: 3.\n",
            "Education entries: 2.\n",
            "\n",
            " Job Requirements Analysis.\n",
            "----------------------------------------\n",
            "Found job skills: ['Ai', 'Generative Ai', 'Autonomous Driving', 'Generative Models', 'Synthetic Data']\n",
            "Found job tools: []\n",
            "Found keywords: ['zoox', 'data', 'using', 'generative', 'model']...\n",
            "New resume generated: datascience_resume_20250805_225420.html\n",
            "Name: CASSANDRA M. SULLIVAN\n",
            "Experience entries: 3\n",
            "Education entries: 2\n",
            "Projects: 5\n",
            "Skill categories: Multiple\n",
            "\n",
            " Interview Prep.\n",
            "----------------------------------------\n",
            "New resume: datascience_resume_20250805_225420.html\n",
            "3 experience entries extracted.\n",
            "5 projects extracted.\n",
            "2 education entries extracted.\n",
            "7 interview talking points generated.\n",
            "\n",
            " Interview talking points.\n",
            "--------------------------------------------------\n",
            "\n",
            "1. Technical Deep Dive: Prepare a detailed walkthrough of your Ai, Generative Ai, Autonomous Driving experience. Use your Healthcare LLM project as an example, explain the technical approach, challenges with EHR data, and how you implemented Reinforcement Learning for fine-tuning.\n",
            "\n",
            "2. Problem-Solving Examples: Prepare STAR method stories for: 1) Your Mexico Central Bank forecasting models that improved predictions by 15%, what obstacles did you overcome? 2) The Universidad KPI dashboard that saved $500K MXN, how did you identify the key metrics? 3) A time when your initial ML approach failed and how you pivoted.\n",
            "\n",
            "3. Career Progression Story: Highlight your growth from Financial Planning Analyst to ML & Data Science Economist. Emphasize: increasing technical complexity (R forecasting → Python ML pipelines → Generative AI), expanding scope (budget accuracy → policy impact → healthcare applications), and leadership development.\n",
            "\n",
            "4. Stakeholder Communication: Prepare examples of: • Presenting complex ML findings to policy makers at Mexico Central Bank • Translating technical limitations to university administrators • Working with cross-departmental teams on KPI dashboards • How you balance technical accuracy with business needs.\n",
            "\n",
            "5. Autonomous Systems Insight: Connect your generative AI and ML pipeline experience to autonomous driving challenges. Discuss data quality issues you've solved, model validation approaches from your forecasting work, and how your A/B testing expertise applies to autonomous system evaluation.\n",
            "\n",
            "6. Quantified Business Impact: Prepare specific metrics: • 15% improvement in economic predictions (Mexico Central Bank) • $500K MXN operational savings (Universidad) • 23% budget accuracy improvement (Treasury) • 6.75M+ records analyzed (Customer Behavior project) • Multiple ongoing research projects showing sustained impact.\n",
            "\n",
            "7. Strategic Questions: Ask thoughtful questions like: • 'What are the biggest challenges in scaling generative models for autonomous systems?' • 'How does the team balance research exploration with production requirements?' • 'What's your approach to ensuring safety and reliability in ML model deployment?' • 'How do you measure success for generative AI initiatives in this domain?'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ResumeSection(name='CASSANDRA M. SULLIVAN', email='caseymr96@gmail.com', phone='(415) 286-1896', summary='Data scientist with proven expertise in machine learning and generative AI, skilled in developing automated evaluation pipelines and fine-tuning LLMs. Brings robust experience in Python, ML/NLP libraries, and A/B testing to optimize model performance at scale. Recognized for translating complex datasets into actionable, real-world insights and integrating advanced ML techniques within quality assurance frameworks.', experience=[{'title': 'Machine Learning & Data Science Economist', 'company': 'Mexico Central Bank', 'duration': 'May 2022 - Sep 2024', 'achievements': ['Developed forecasting models (XGBoost, ARIMA, Prophet) that improved regional economic predictions by 15%.', 'Applied causal inference (DiD, matching) and A/B testing to evaluate the impact of policy interventions.', 'Conducted incrementality analysis to isolate true effects of programs on regional growth.', 'Automated SQL pipelines while integrating generative AI tools to streamline data evaluation processes.', 'Presented findings in policy briefings and national economic reports.']}, {'title': 'Consultant', 'company': 'Universidad Autónoma de Nuevo León (University-Industry Relations Office)', 'duration': 'Sep 2021 - May 2022', 'achievements': ['Designed and deployed Python-based KPI dashboards to track cross-departmental performance, contributing to $500K MXN in operational savings.', 'Leveraged A/B testing and uplift modeling in data analysis to isolate the incremental effects of pricing strategies on customer conversion.']}, {'title': 'Financial Planning Analyst', 'company': 'Secretary of Finance and General Treasury of Nuevo León (State Gov. Office)', 'duration': 'Jun 2020 - Aug 2021', 'achievements': ['Developed R-based forecasting models that improved budget accuracy by 23%.', 'Automated SQL pipelines to streamline debt tracking, reducing servicing costs by 0.6%.', 'Utilized causal inference (synthetic controls, quasi-experiments) to evaluate the impact of fiscal reforms.', 'Facilitated evidence-based decision-making through real-time financial reporting.']}], education=[{'school': 'University of Chicago, Physical Sciences Division', 'degree': 'Master of Science, Applied Data Science', 'date': 'Dec 2025', 'details': 'GPA: Magna Cum Laude • Data Science Institute Merit Scholarship'}, {'school': 'Instituto Tecnológico y de Estudios Superiores de Monterrey', 'degree': 'Bachelor of Arts, Economics', 'date': 'Dec 2019', 'details': 'GPA: Magna Cum Laude'}], skills=['Python', 'R', 'SQL', 'STATA', 'Git', 'Jupyter', 'Pandas', 'NumPy', 'Matplotlib', 'Scikit-Learn', 'Machine Learning', 'Deep Learning', 'Generative AI', 'LLMs', 'VAEs', 'Classification', 'Regression', 'XGBoost', 'Random Forest', 'A/B Testing', 'NLP', 'TensorFlow', 'PyTorch', 'Statistics', 'Bayesian Inference', 'Causal Inference', 'Time Series Forecasting', 'Tableau', 'Streamlit'], projects=[{'name': 'Healthcare LLM (Inference Analytics)', 'duration': 'Mar 2025 - Present', 'description': 'Collaborating on a healthcare-specialized LLM fine-tuned with Reinforcement Learning from EHRs and clinical notes. Focused on prompt engineering, dataset preparation, and reward modeling.', 'technologies': ['Python', 'Reinforcement Learning', 'LLM', 'EHRs']}, {'name': 'AirfareCast: Airline Fare Forecasting', 'duration': 'Mar 2025', 'description': 'Built machine learning models (XGBoost, Random Forest) to predict flight prices. Deployed an interactive Streamlit dashboard to visualize fare trends and optimize booking decisions.', 'technologies': ['Python', 'XGBoost', 'Random Forest', 'Streamlit']}, {'name': 'Conditional VAE for Age-Controlled Face Generation', 'duration': 'Jan 2025', 'description': 'Designed a Conditional Variational Autoencoder (CVAE) to simulate age-based facial transformations. Implemented data preprocessing and achieved latent space disentanglement.', 'technologies': ['Python', 'CVAE', 'TensorFlow', 'Computer Vision']}, {'name': 'Customer Behavior Analysis Pipeline', 'duration': 'Nov 2024', 'description': 'Built a SQL-Python pipeline to analyze 6.75M+ e-commerce records using A/B testing and time series analysis.', 'technologies': ['Python', 'SQL', 'A/B Testing', 'Time Series']}, {'name': 'John List Voltage Research Program', 'duration': 'Sep 2024 - Present', 'description': 'Used Bayesian modeling and causal inference to quantify real-world treatment effects in behavioral experiments, evaluated incremental outcomes from randomized interventions.', 'technologies': ['Python', 'Bayesian Modeling', 'Causal Inference', 'R']}]),\n",
              " {'skills': ['Ai',\n",
              "   'Generative Ai',\n",
              "   'Autonomous Driving',\n",
              "   'Generative Models',\n",
              "   'Synthetic Data'],\n",
              "  'tools': [],\n",
              "  'keywords': ['zoox',\n",
              "   'data',\n",
              "   'using',\n",
              "   'generative',\n",
              "   'model',\n",
              "   'training',\n",
              "   'autonomous',\n",
              "   'driving',\n",
              "   'models',\n",
              "   'scale',\n",
              "   'scenarios',\n",
              "   'safety']},\n",
              " 'datascience_resume_20250805_225420.html')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zSuZl6jdkgQ",
        "outputId": "bb115155-6bd6-4579-e500-b056ef3ec4ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import difflib\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# My original resume in PDF.\n",
        "with open(\"Cassandra_Sullivan_DataScience.pdf\", \"rb\") as f:\n",
        "    import fitz\n",
        "    doc = fitz.open(stream=f.read(), filetype=\"pdf\")\n",
        "    original_text = \"\\n\".join([page.get_text() for page in doc])\n",
        "\n",
        "# Data science tailored resume in html.\n",
        "with open(\"datascience_resume_20250805_225420.html\", \"r\", encoding=\"utf-8\") as f:\n",
        "    soup = BeautifulSoup(f, \"html.parser\")\n",
        "    tailored_text = soup.get_text()\n",
        "\n",
        "# Unified diff.\n",
        "diff = difflib.unified_diff(\n",
        "    original_text.splitlines(),\n",
        "    tailored_text.splitlines(),\n",
        "    fromfile=\"Original Resume (PDF)\",\n",
        "    tofile=\"Tailored Resume (HTML)\",\n",
        "    lineterm=\"\"\n",
        ")\n",
        "\n",
        "\n",
        "diff_output = list(diff)\n",
        "print(\"\\n\".join(diff_output[:300]) or \"No changes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caf5jZKZdMz3",
        "outputId": "497886ae-02ef-4884-c5c9-4c95ab614c2c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Original Resume (PDF)\n",
            "+++ Tailored Resume (HTML)\n",
            "@@ -1,68 +1,92 @@\n",
            "- \n",
            "- \n",
            "-CASSANDRA M. SULLIVAN \n",
            "-+ 1 (415) 286-1896 | cassandramr@uchicago.edu | in/cassandra-msullivan |  Github\n",
            "-Data scientist with experience in generative AI and large-scale ML systems. I’ve built A/B testing frameworks and automated \n",
            "-evaluation pipelines that made it easier to track model performance and translate results into decisions in research, product or \n",
            "-policy. \n",
            "-WORK EXPERIENCE \n",
            "-Mexico Central Bank (Banco de Mexico) \n",
            "-May 2022 - Sep 2024 \n",
            "-Machine Learning & Data Science Economist                                                                                                                Monterrey, Mexico \n",
            "-• Developed forecasting models (XGBoost, ARIMA, Prophet) that improved regional economic predictions by 15%. \n",
            "-• Applied causal inference (DiD, matching) and A/B testing to evaluate the impact of policy interventions. \n",
            "-• Conducted incrementality analysis to isolate true effects of programs on regional growth. \n",
            "-• Automated SQL pipelines while integrating generative AI tools to streamline data evaluation processes, aligning with large-\n",
            "-scale model automation principles. \n",
            "-• My analyses were featured in national economic reports and policy briefings that informed high-level decisions and reached \n",
            "-a nationwide audience. \n",
            "-Universidad Autónoma de Nuevo León (University-Industry Relations Office) \n",
            "-Sep 2021 - May 2022 \n",
            "-Consultant                                                                                                                                                                        Monterrey, Mexico \n",
            "-• Built Python-based KPI dashboards that improved visibility across departments and helped reduce operating costs by a 7% \n",
            "-annually. \n",
            "-• Leveraged A/B testing and uplift modeling in data analysis to isolate the incremental effects of pricing strategies on \n",
            "-customer conversion. \n",
            "-Secretary of Finance and General Treasury of Nuevo León (State Gov. Office) \n",
            "-Jun 2020 - Aug 2021 \n",
            "-Financial Planning Analyst \n",
            "-Mexico \n",
            "-• Improved budget accuracy by 23% through R-based forecasting models and real-time financial reporting tools. \n",
            "-• Reduced debt servicing costs by 1% by automating SQL pipelines and applying causal inference to evaluate fiscal reforms. \n",
            "-EDUCATION \n",
            "-University of Chicago, Physical Sciences Division \n",
            "-Dec 2025 \n",
            "-Master in Applied Data Science                                                                                                                                              Chicago, IL \n",
            "-• GPA: Magna Cum Laude \n",
            "-• Achievements: Data Science Institute Merit Scholarship. \n",
            "-Instituto Tecnológico y de Estudios Superiores de Monterrey \n",
            "-Dec 2019 \n",
            "-Bachelor in Economics                                                                                                                                                  Monterrey, Mexico \n",
            "-• GPA: Magna Cum Laude 94/100 \n",
            "-• Achievements: Academic Excellence Award (Top 5%), President of the 36th Edition of the Economics Symposium. \n",
            "-MAJOR PERSONAL PROJECTS \n",
            "-Healthcare LLM (Inference Analytics) \n",
            "-Mar 2025 - Present \n",
            "-• Collaborating on a healthcare-specialized LLM fine-tuned with Reinforcement Learning from EHRs and clinical notes. \n",
            "-Focused on prompt engineering, dataset preparation, and reward modeling. \n",
            "-AirfareCast: Airline Fare Forecasting \n",
            "-Mar 2025 \n",
            "-• Built machine learning models (XGBoost, Random Forest) to predict flight prices. Deployed an interactive Streamlit \n",
            "-dashboard to visualize fare trends and optimize booking decisions. \n",
            "-Conditional VAE for Age-Controlled Face Generation \n",
            "-Jan 2025 \n",
            "-• Designed a Conditional Variational Autoencoder (CVAE) to simulate age-based facial transformations. Implemented data \n",
            "-preprocessing and achieved latent space disentanglement. \n",
            "-Customer Behavior Analysis Pipeline \n",
            "-Nov 2024 \n",
            "-• Built a SQL-Python pipeline to analyze 6.75M+ e-commerce records using A/B testing and time series. \n",
            "-John List Voltage Research Program \n",
            "-Sep 2024 - Present \n",
            "-• Used Bayesian modeling and causal inference to quantify real-world treatment effects in behavioral experiments, evaluated \n",
            "-incremental outcomes from randomized interventions. \n",
            "-TECHNICAL SKILLS \n",
            "-• Programming: Python (Pandas, NumPy, Matplotlib, Scikit-Learn), SQL, R, STATA, Git. \n",
            "-• Machine Learning & AI: Generative AI (LLMs, VAEs), Classification, Regression, XGBoost, Random Forest, A/B \n",
            "-Testing, RNN-LSTM, NLP, Neural Networks, Prompt Engineering. \n",
            "-• Statistics: Time Series Forecasting, Bayesian Inference, GLMs, MLE, MCMC, Panel Data Econometrics, Causal \n",
            "-Inference, Sentiment Analysis.\n",
            "+\n",
            "+\n",
            "+\n",
            "+\n",
            "+\n",
            "+CASSANDRA M. SULLIVAN - Resume\n",
            "+\n",
            "+\n",
            "+\n",
            "+\n",
            "+\n",
            "+CASSANDRA M. SULLIVAN\n",
            "+(415) 286-1896 | caseymr96@gmail.com\n",
            "+\n",
            "+🔗 GitHub\n",
            "+🔗 LinkedIn\n",
            "+\n",
            "+\n",
            "+\n",
            "+\n",
            "+Professional Summary\n",
            "+Data scientist with proven expertise in machine learning and generative AI, skilled in developing automated evaluation pipelines and fine-tuning LLMs. Brings robust experience in Python, ML/NLP libraries, and A/B testing to optimize model performance at scale. Recognized for translating complex datasets into actionable, real-world insights and integrating advanced ML techniques within quality assurance frameworks.\n",
            "+\n",
            "+\n",
            "+Professional Experience\n",
            "+\n",
            "+Machine Learning & Data Science Economist\n",
            "+Mexico Central Bank\n",
            "+May 2022 - Sep 2024Developed forecasting models (XGBoost, ARIMA, Prophet) that improved regional economic predictions by 15%.Applied causal inference (DiD, matching) and A/B testing to evaluate the impact of policy interventions.Conducted incrementality analysis to isolate true effects of programs on regional growth.Automated SQL pipelines while integrating generative AI tools to streamline data evaluation processes.Presented findings in policy briefings and national economic reports.\n",
            "+\n",
            "+Consultant\n",
            "+Universidad Autónoma de Nuevo León (University-Industry Relations Office)\n",
            "+Sep 2021 - May 2022Designed and deployed Python-based KPI dashboards to track cross-departmental performance, contributing to $500K MXN in operational savings.Leveraged A/B testing and uplift modeling in data analysis to isolate the incremental effects of pricing strategies on customer conversion.\n",
            "+\n",
            "+Financial Planning Analyst\n",
            "+Secretary of Finance and General Treasury of Nuevo León (State Gov. Office)\n",
            "+Jun 2020 - Aug 2021Developed R-based forecasting models that improved budget accuracy by 23%.Automated SQL pipelines to streamline debt tracking, reducing servicing costs by 0.6%.Utilized causal inference (synthetic controls, quasi-experiments) to evaluate the impact of fiscal reforms.Facilitated evidence-based decision-making through real-time financial reporting.\n",
            "+\n",
            "+Education\n",
            "+\n",
            "+Master of Science, Applied Data Science\n",
            "+University of Chicago, Physical Sciences Division\n",
            "+Dec 2025GPA: Magna Cum Laude • Data Science Institute Merit Scholarship\n",
            "+\n",
            "+Bachelor of Arts, Economics\n",
            "+Instituto Tecnológico y de Estudios Superiores de Monterrey\n",
            "+Dec 2019GPA: Magna Cum Laude\n",
            "+\n",
            "+Key Projects\n",
            "+\n",
            "+Healthcare LLM (Inference Analytics)Mar 2025 - PresentCollaborating on a healthcare-specialized LLM fine-tuned with Reinforcement Learning from EHRs and clinical notes. Focused on prompt engineering, dataset preparation, and reward modeling.\n",
            "+Technologies:\n",
            "+Python, Reinforcement Learning, LLM, EHRs\n",
            "+\n",
            "+\n",
            "+AirfareCast: Airline Fare ForecastingMar 2025Built machine learning models (XGBoost, Random Forest) to predict flight prices. Deployed an interactive Streamlit dashboard to visualize fare trends and optimize booking decisions.\n",
            "+Technologies:\n",
            "+Python, XGBoost, Random Forest, Streamlit\n",
            "+\n",
            "+\n",
            "+Conditional VAE for Age-Controlled Face GenerationJan 2025Designed a Conditional Variational Autoencoder (CVAE) to simulate age-based facial transformations. Implemented data preprocessing and achieved latent space disentanglement.\n",
            "+Technologies:\n",
            "+Python, CVAE, TensorFlow, Computer Vision\n",
            "+\n",
            "+\n",
            "+Customer Behavior Analysis PipelineNov 2024Built a SQL-Python pipeline to analyze 6.75M+ e-commerce records using A/B testing and time series analysis.\n",
            "+Technologies:\n",
            "+Python, SQL, A/B Testing, Time Series\n",
            "+\n",
            "+\n",
            "+John List Voltage Research ProgramSep 2024 - PresentUsed Bayesian modeling and causal inference to quantify real-world treatment effects in behavioral experiments, evaluated incremental outcomes from randomized interventions.\n",
            "+Technologies:\n",
            "+Python, Bayesian Modeling, Causal Inference, R\n",
            "+\n",
            "+\n",
            "+Technical Skills\n",
            "+\n",
            "+\n",
            "+Programming & Scripting\n",
            "+Time Series ForecastingClassificationStreamlitMatplotlibGitSTATAGenerative AIPandasDeep LearningRegressionNumPyBayesian InferenceJupyterScikit-LearnCausal InferencePythonPyTorchTensorFlowVAEsRandom ForestMachine LearningSQL\n",
            "+\n",
            "+Machine Learning & AI\n",
            "+LLMsNLPXGBoost\n",
            "+\n",
            "+Data Analysis & Visualization\n",
            "+Tableau\n",
            "+\n",
            "+Statistical Methods\n",
            "+A/B TestingStatistics\n",
            "+\n",
            "+\n",
            "+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Resume Changes\n",
        "\n",
        "These modifications were made to my resume for the Apple Ads – Data Scientist, Research Analytics role:\n",
        "\n",
        "### Structural & Visual Enhancements\n",
        "- Improved the resume format using HTML with sections, categorized skill tags and a more readable layout.\n",
        "- Improved skimmability with bolded headers, bullet groupings and cleaner spacing.\n",
        "\n",
        "### Professional Summary\n",
        "- Rewritten to highlight ML/LLM expertise, A/B testing and the ability to translate data into operational impact.\n",
        "- Expanded from 2 lines to a detailed summary focused on automated pipelines, GEN AI and QA integration.\n",
        "\n",
        "### Experience Section\n",
        "- Removed redundant phrasing like location headers.\n",
        "- Quantified outcomes more clearly, for example went from $500K MXN in savings to a 0.6% cost reduction.\n",
        "- Made achievements more results and impact oriented.\n",
        "- Removed phrasing like aligning with large-scale model automation principles to streamline bullets.\n",
        "\n",
        "### Education\n",
        "- Degree names reformatted from Master in Applied Data Science to Master of Science, Applied Data Science for consistency with U.S. academic formats.\n",
        "- Location information was removed.\n",
        "\n",
        "### Projects\n",
        "- Grouped technologies used into clearly labeled stacks.\n",
        "- Added concise project descriptions.\n",
        "- Keept all the original projects but improved their language.\n",
        "\n",
        "### Technical Skills\n",
        "- Split and categorized technical skills into four areas:\n",
        "  - Programming & Scripting\n",
        "  - ML & AI\n",
        "  - Visualization\n",
        "  - Statistical Methods\n",
        "- Replaced bullet lists with tag type labels, improving ATS compatibility.\n",
        "\n",
        "### Contact\n",
        "- Added LinkedIn and GitHub links with visual icons.\n",
        "- Used a header and contact info in a cleaner design format.\n",
        "\n",
        "These refinements align my resume with Apple’s expectations for experimentation, LLM experience, model automation and business impact communication."
      ],
      "metadata": {
        "id": "WloQv6xjea8K"
      }
    }
  ]
}